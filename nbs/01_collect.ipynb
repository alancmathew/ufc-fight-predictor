{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b529969-d1f5-49b5-bea8-f36f33bf66d0",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f86d6700-e4e7-463d-b00e-5fe7771b30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3967c36b-4763-4ac9-95d6-1b6e7933fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac3ab9c-f5e1-47e9-a9eb-59ad64e39ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045710fb-2ecb-49f0-9f74-44f79e437b4d",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b139c4df-18da-43ff-9638-c1bf10f81f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventsCollector:\n",
    "        \n",
    "    def save_eventlist_pages(self):\n",
    "        completed_first_url = \"http://ufcstats.com/statistics/events/completed?page=1\"\n",
    "        upcoming_first_url = \"http://ufcstats.com/statistics/events/upcoming?page=1\"\n",
    "        \n",
    "        download_sequential_pages(completed_first_url, \n",
    "                                  dir_dict[\"completed_eventlist_html\"])\n",
    "        download_sequential_pages(upcoming_first_url, \n",
    "                                  dir_dict[\"upcoming_eventlist_html\"])\n",
    "         \n",
    "    def get_events_list(self, eventlist_html_dir: str) -> list[str]:\n",
    "    \n",
    "        def extract_event_data(row):\n",
    "            features = {\n",
    "                \"name\": None,\n",
    "                \"date\": None,\n",
    "                \"location\": None,\n",
    "                \"url\": None\n",
    "            }\n",
    "\n",
    "            a_elem = row.find(\"a\", class_=\"b-link b-link_style_black\")\n",
    "            features[\"name\"], features[\"url\"] = a_elem.text.strip(), a_elem[\"href\"]\n",
    "            features[\"date\"] = row.find(\"span\", class_=\"b-statistics__date\").text.strip()\n",
    "            features[\"location\"] = row.find_all(\"td\")[1].text.strip()\n",
    "\n",
    "            return features\n",
    "\n",
    "        events_list = []\n",
    "        page_files = sorted(lfilter(lambda s: s.endswith(\".html\"), \n",
    "                                    os.listdir(eventlist_html_dir)))\n",
    "        for file in page_files:\n",
    "            filepath = os.path.join(eventlist_html_dir, file)\n",
    "            with open(filepath, \"r\") as f:\n",
    "                eventlistpage_html = f.read()\n",
    "                assert eventlistpage_html != \"\"\n",
    "                soup = BeautifulSoup(eventlistpage_html, features=\"lxml\")\n",
    "                row_elems = soup.find(\"tbody\").find_all(\"tr\", class_=\"b-statistics__table-row\")\n",
    "                row_elems = lfilter(lambda r: \\\n",
    "                                    len(r.find_all(\"a\", \n",
    "                                                   class_=\"b-link b-link_style_black\")) != 0, \\\n",
    "                                                row_elems)\n",
    "                events_sublist = lmap(extract_event_data, row_elems)\n",
    "                events_list.extend(events_sublist)\n",
    "\n",
    "        return events_list\n",
    "    \n",
    "    def get_eventlist_df(self, eventlist_html_dir: str, outfilename: str) -> pd.DataFrame:\n",
    "        events_list = self.get_events_list(eventlist_html_dir)\n",
    "        print(f\"Event list length: {len(events_list)}\")\n",
    "        events_df = pd.DataFrame(events_list)\n",
    "        filepath = os.path.join(dir_dict[\"raw_csv\"], outfilename)\n",
    "        events_df.to_csv(filepath, index=False)\n",
    "        return events_df\n",
    "    \n",
    "    \n",
    "    def get_eventlist_dfs(self):\n",
    "        self.completed_events_df = \\\n",
    "                self.get_eventlist_df(dir_dict[\"completed_eventlist_html\"], \n",
    "                                      \"completed_events.csv\")\n",
    "        \n",
    "        self.upcoming_events_df = \\\n",
    "                self.get_eventlist_df(dir_dict[\"upcoming_eventlist_html\"], \n",
    "                                      \"upcoming_events.csv\")\n",
    "\n",
    "        \n",
    "    def save_event_pages(self):\n",
    "        completed_event_urls = self.completed_events_df[\"url\"].to_list()\n",
    "        save_pages(completed_event_urls, dir_dict[\"completed_events_html\"])\n",
    "        \n",
    "        upcoming_event_urls = self.upcoming_events_df[\"url\"].to_list()\n",
    "        save_pages(upcoming_event_urls, dir_dict[\"upcoming_events_html\"])\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        self.save_eventlist_pages()\n",
    "        self.get_eventlist_dfs()\n",
    "        self.save_event_pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf70345-97fc-4223-a265-1b13faad5df8",
   "metadata": {},
   "source": [
    "## Fights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89be06b2-5662-4147-bb2e-6f42b455ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FightsCollector:\n",
    "    \n",
    "    def extract_fight_urls(self, event_html_filepath: str) -> list[str]:\n",
    "    \n",
    "        with open(event_html_filepath, \"r\") as f:\n",
    "            html_str = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_str, features=\"lxml\")\n",
    "        table = soup.find(\"table\")\n",
    "\n",
    "        links_list = []\n",
    "\n",
    "        headers = [key for key in map(lambda x: x.text.strip(), \n",
    "                                      table.find(\"thead\").find_all(\"th\"))]\n",
    "\n",
    "        rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "\n",
    "        for row in rows:\n",
    "            for col, elem in zip(headers, row.find_all(\"td\")):\n",
    "                if col == \"Weight class\":\n",
    "                    p_elem = elem.find(\"p\")\n",
    "                    val = p_elem.text.strip()\n",
    "                    links_list.append((row[\"data-link\"], val))\n",
    "\n",
    "        return links_list\n",
    "    \n",
    "    def save_fight_urls(self, events_html_dir: str, outfilename: str) -> list[str]:\n",
    "        event_files = lfilter(lambda x: x.endswith(\".html\"), os.listdir(events_html_dir))\n",
    "\n",
    "        fight_urls_list = []\n",
    "        for filename in event_files:\n",
    "            filepath = os.path.join(events_html_dir, filename)\n",
    "            fight_urls_sublist = self.extract_fight_urls(filepath)\n",
    "            fight_urls_list.extend(fight_urls_sublist)\n",
    "\n",
    "        print(f\"Fight urls list length: {len(fight_urls_list)}\")\n",
    "        df = pd.DataFrame(fight_urls_list, columns=[\"Fight Url\", \"Weight Class\"])\n",
    "        filepath = os.path.join(dir_dict[\"raw_csv\"], outfilename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        return df[\"Fight Url\"].to_list()\n",
    "    \n",
    "    def get_fight_urls(self):\n",
    "        self.completed_fight_urls_list = \\\n",
    "            self.save_fight_urls(dir_dict[\"completed_events_html\"], \n",
    "                                 \"completed_fight_urls_weightclasses.csv\")\n",
    "        \n",
    "        self.upcoming_fight_urls_list = \\\n",
    "            self.save_fight_urls(dir_dict[\"upcoming_events_html\"], \n",
    "                                 \"upcoming_fight_urls_weightclasses.csv\")\n",
    "        \n",
    "    def save_fight_pages(self):\n",
    "        save_pages(self.completed_fight_urls_list, dir_dict[\"completed_fights_html\"])\n",
    "        save_pages(self.upcoming_fight_urls_list, dir_dict[\"upcoming_fights_html\"])\n",
    "        \n",
    "    \n",
    "    def indiv_fight_data_extractor(self, fight_id_html, is_upcoming=False):\n",
    "        fight_id, fight_html = fight_id_html \n",
    "        fight_dict = {}\n",
    "        \n",
    "        soup = BeautifulSoup(fight_html, features=\"lxml\")\n",
    "        \n",
    "        title_elem = soup.find(\"h2\", class_=\"b-content__title\")\n",
    "        title_a_elem = title_elem.find(\"a\")\n",
    "        fight_dict[\"Event Name\"] = title_a_elem.text.strip()\n",
    "        fight_dict[\"Event Url\"] = title_a_elem[\"href\"]\n",
    "        fight_dict[\"Fight ID\"] = fight_id\n",
    "        fighters = soup.find_all(\"div\", class_=\"b-fight-details__person\")\n",
    "        for idx, fighter in enumerate(fighters, start=1):\n",
    "            if not is_upcoming:\n",
    "                fight_dict[f\"Fighter{idx} Status\"] = fighter.find(\"i\", \n",
    "                                        class_=\"b-fight-details__person-status\").text.strip()\n",
    "\n",
    "            a_elem = fighter.find(\"a\", class_=\"b-link b-fight-details__person-link\")\n",
    "            fight_dict[f\"Fighter{idx} Name\"] = a_elem.text.strip()\n",
    "            fight_dict[f\"Fighter{idx} Url\"] = a_elem[\"href\"]\n",
    "\n",
    "        fight_dict[\"Bout\"] = soup.find(\"i\", class_=\"b-fight-details__fight-title\").text.strip()\n",
    "\n",
    "        if is_upcoming:\n",
    "            return fight_dict\n",
    "        \n",
    "        fight_details_div = soup.find(\"div\", class_=\"b-fight-details__content\")\n",
    "\n",
    "        method_elem = fight_details_div.find(\"p\").find(\"i\", \n",
    "                                                       class_=\"b-fight-details__text-item_first\")\n",
    "\n",
    "        def get_details(i_elem):\n",
    "            i_text = i_elem.text.replace(\"\\n\", \"\").strip()\n",
    "            m = re.search(r\"(.*):\\s+(.*)\", i_text)\n",
    "            if m:\n",
    "                return (m.group(1), m.group(2))\n",
    "\n",
    "        label_elems = fight_details_div.find_all(\"i\", class_=\"b-fight-details__label\")\n",
    "        detail_elems = lmap(lambda e: e.parent, label_elems)\n",
    "        detail_tups = lfilter(lambda t: t != None, map(get_details, detail_elems))\n",
    "\n",
    "        for label, text in detail_tups:\n",
    "            fight_dict[label] = text\n",
    "\n",
    "        details_text = fight_details_div.find_all(\"p\")[1].text.replace(\"\\n\", \"\").strip()\n",
    "\n",
    "        m = re.search(r\"(.*):\\s+(.*)\", details_text)\n",
    "        if m:\n",
    "            fight_dict[m.group(1)] = m.group(2)\n",
    "\n",
    "        tables = soup.find_all(\"table\")\n",
    "\n",
    "        def fight_tables_to_dicts(page_html: str):\n",
    "\n",
    "            def extract_table(tables):\n",
    "                data_dict = {key:[] for key in map(lambda x: x.text.strip(), \\\n",
    "                                                       tables[0].find(\"thead\").find_all(\"th\"))}\n",
    "                data_dict[\"Round\"] = []\n",
    "\n",
    "                for i, table in enumerate(tables[:2]):\n",
    "                    rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "                    for j, row in enumerate(rows, start=1):\n",
    "                        for col, elem in zip(data_dict.keys(), row.find_all(\"td\")):\n",
    "                            if col == \"Fighter\":\n",
    "                                for a_elem in elem.find_all(\"a\"):\n",
    "                                    data_dict[col].append(a_elem.text.strip())\n",
    "                            else:\n",
    "                                for p_elem in elem.find_all(\"p\"):\n",
    "                                    val = p_elem.text.strip()\n",
    "                                    val = val if val != \"---\" else None\n",
    "                                    data_dict[col].append(val)\n",
    "\n",
    "                        if i == 0:\n",
    "                            data_dict[\"Round\"].extend([\"Overall\", \"Overall\"])\n",
    "                        else:\n",
    "                            data_dict[\"Round\"].extend([f\"Round {j}\", f\"Round {j}\"])\n",
    "\n",
    "\n",
    "                return data_dict\n",
    "\n",
    "\n",
    "            soup = BeautifulSoup(page_html, features=\"lxml\")\n",
    "            tables = soup.find_all(\"table\")\n",
    "\n",
    "            return extract_table(tables[:2]), extract_table(tables[2:])\n",
    "\n",
    "\n",
    "        try:\n",
    "            fight_dict[\"Totals\"], fight_dict[\"Significant Strikes\"] = \\\n",
    "                                                    fight_tables_to_dicts(fight_html)\n",
    "        except IndexError:\n",
    "            pass \n",
    "\n",
    "        return fight_dict\n",
    "    \n",
    "    def all_fight_data_extractor(self, identifier, fights_html_dir):\n",
    "        fights_html_dict = {}\n",
    "\n",
    "        fight_files = lfilter(lambda x: x.endswith(\".html\"), os.listdir(fights_html_dir))\n",
    "        for filename in fight_files:\n",
    "            filepath = os.path.join(fights_html_dir, filename)\n",
    "            with open(filepath, \"r\") as f:\n",
    "                html_str = f.read()\n",
    "                fight_id = filename.replace(\".html\",\"\")\n",
    "                fights_html_dict[fight_id] = html_str\n",
    "\n",
    "        with Pool(cpu_count()) as p:\n",
    "            if identifier == \"upcoming\":\n",
    "                self.indiv_fight_data_extractor = partial(self.indiv_fight_data_extractor, \n",
    "                                                          is_upcoming=True)\n",
    "            fights_dict_list = p.map(self.indiv_fight_data_extractor, fights_html_dict.items())\n",
    "\n",
    "        return fights_dict_list\n",
    "        \n",
    "    def get_fight_dict(self, identifier, fights_html_dir): \n",
    "        fights_dict_list = \\\n",
    "                        self.all_fight_data_extractor(identifier, fights_html_dir)\n",
    "        filepath = os.path.join(dir_dict[\"raw_json\"], f\"{identifier}_fights.json\")\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(fights_dict_list, f, indent=4)\n",
    "            \n",
    "        return fights_dict_list\n",
    "    \n",
    "    \n",
    "    def process_fight_dict(self, fight_dict):\n",
    "\n",
    "        def merge_additional_data(data_header):\n",
    "            additional_dict = fight_dict.pop(data_header)\n",
    "            fighter_names = additional_dict[\"Fighter\"]\n",
    "            round_order = lmap(lambda x: x.replace(\" \", \"\"), additional_dict[\"Round\"])\n",
    "            if (fighter_names[0] == fight_dict[\"Fighter1 Name\"]) and \\\n",
    "                                (fighter_names[1] == fight_dict[\"Fighter2 Name\"]):\n",
    "                fighter_order = [\"Fighter1\", \"Fighter2\"]\n",
    "            elif (fighter_names[0] == fight_dict[\"Fighter2 Name\"]) and \\\n",
    "                                (fighter_names[1] == fight_dict[\"Fighter1 Name\"]):\n",
    "                fighter_order = [\"Fighter1\", \"Fighter2\"]\n",
    "            else:\n",
    "                raise Exception(f\"Fighter names not congurent in {data_header} table\")\n",
    "\n",
    "            for key, lov in list(additional_dict.items())[1:-1]:\n",
    "                for i, v in enumerate(lov):\n",
    "                    var = f\"{key}_SS\" if data_header == \"Significant Strikes\" else key\n",
    "                    fight_dict[f\"{fighter_order[(i % 2)]}_{round_order[i]}_{var}\"] = v\n",
    "\n",
    "        fight_dict_keys = fight_dict.keys()\n",
    "        additional_data_headers = [\"Totals\", \"Significant Strikes\"]\n",
    "        for data_header in additional_data_headers:\n",
    "            if data_header in fight_dict_keys:\n",
    "                merge_additional_data(data_header)\n",
    "\n",
    "        return fight_dict\n",
    "\n",
    "    def fight_dicts_to_df(self, fight_dict_list):\n",
    "        with Pool(cpu_count()) as p:\n",
    "            res_fight_dict_list = p.map(self.process_fight_dict, fight_dict_list)\n",
    "\n",
    "        return pd.DataFrame(res_fight_dict_list)\n",
    "\n",
    "    \n",
    "    def save_fight_df(self, identifier):\n",
    "        if identifier == \"completed\":\n",
    "            folderpath = dir_dict[\"completed_fights_html\"]\n",
    "        elif identifier == \"upcoming\":\n",
    "            folderpath = dir_dict[\"upcoming_fights_html\"]\n",
    "        else:\n",
    "            raise(\"Unknown file name in save_fight_df\")\n",
    "            \n",
    "        fights_lod = self.get_fight_dict(identifier, folderpath)\n",
    "        fights_df = self.fight_dicts_to_df(fights_lod)\n",
    "        filepath = os.path.join(dir_dict[\"raw_csv\"], f\"{identifier}_fights.csv\")\n",
    "        fights_df.to_csv(filepath, index=False)\n",
    "        \n",
    "    def save_fight_dfs(self):\n",
    "        self.save_fight_df(\"completed\")\n",
    "        self.save_fight_df(\"upcoming\")\n",
    "    \n",
    "    def start(self):\n",
    "        self.get_fight_urls()\n",
    "        self.save_fight_pages()\n",
    "        self.save_fight_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c37cc5-bd92-43d2-a4f1-aaf13629251c",
   "metadata": {},
   "source": [
    "## Fighters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab427506-7d95-42e7-98aa-cbaec843a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FightersCollector:\n",
    "        \n",
    "    def save_fighterlist_pages(self):\n",
    "        for i in range(97,123):\n",
    "            fighterlist_page_first_url = f\"http://ufcstats.com/statistics/fighters?char={chr(i)}&page=1\"\n",
    "            download_sequential_pages(fighterlist_page_first_url, dir_dict[\"fighterlist_html\"])\n",
    "        \n",
    "    def extract_fighter_urls(self, fighterlist_html_filepath: str) -> list[str]:\n",
    "        with open(fighterlist_html_filepath, \"r\") as f:\n",
    "            html_str = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_str, features=\"lxml\")\n",
    "        rows = soup.find(\"tbody\").find_all(\"tr\")\n",
    "        rows = filter(lambda r: r.find(\"a\") != None, rows)\n",
    "        return lmap(lambda r: r.find(\"a\")[\"href\"], rows)\n",
    "    \n",
    "    def save_fighter_urls(self, fighterlist_html_dir: str, outfilename: str) -> list[str]:\n",
    "        fighterlist_files = lfilter(lambda x: x.endswith(\".html\"), \n",
    "                                    os.listdir(fighterlist_html_dir))\n",
    "\n",
    "        fighter_urls_list = []\n",
    "        for filename in tqdm(fighterlist_files):\n",
    "            filepath = os.path.join(fighterlist_html_dir, filename)\n",
    "            fighter_urls_sublist = self.extract_fighter_urls(filepath)\n",
    "            fighter_urls_list.extend(fighter_urls_sublist)\n",
    "\n",
    "        print(f\"Fighter urls list length: {len(fighter_urls_list)}\")\n",
    "        filepath = os.path.join(dir_dict[\"raw_csv\"], outfilename)\n",
    "        write_list_to_file(fighter_urls_list, filepath)\n",
    "        return fighter_urls_list\n",
    "    \n",
    "    def get_fighter_urls_list(self):\n",
    "        self.fighters_url_list = self.save_fighter_urls(dir_dict[\"fighterlist_html\"],\n",
    "                                                        \"fighter_urls.txt\")\n",
    "        \n",
    "    def save_fighter_pages(self):\n",
    "        alread_downloaded = lmap(lambda s: \\\n",
    "                                     f\"http://ufcstats.com/fighter-details/{s.replace('.html','')}\", \n",
    "                                 os.listdir(dir_dict[\"fighters_html\"]))\n",
    "        \n",
    "        self.fighters_url_list = set(self.fighters_url_list).difference(alread_downloaded)\n",
    "        \n",
    "        save_pages(self.fighters_url_list, dir_dict[\"fighters_html\"])\n",
    "        \n",
    "        \n",
    "    def extract_fighter_data(self, fighter_page_filepath):\n",
    "\n",
    "        with open(fighter_page_filepath, \"r\") as f:\n",
    "            html_str = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html_str, features=\"lxml\")\n",
    "\n",
    "        fighter_dict = dict()\n",
    "\n",
    "        fighter_dict[\"Name\"] = soup.find(\"span\", \n",
    "                                         class_=\"b-content__title-highlight\").text.strip() \n",
    "\n",
    "        fighter_dict[\"Fighter ID\"] = os.path.split(fighter_page_filepath)[1].replace(\".html\",\"\")\n",
    "\n",
    "        fighter_dict[\"Record\"] = soup.find(\"span\", \n",
    "                        class_=\"b-content__title-record\").text.replace(\"Record:\",\"\").strip() \n",
    "\n",
    "        nn = soup.find(\"p\", class_=\"b-content__Nickname\").text.strip()\n",
    "        fighter_dict[\"Nickname\"] = nn if nn else None\n",
    "\n",
    "        fighter_dict\n",
    "\n",
    "        uls = soup.find_all(\"ul\", class_=\"b-list__box-list\")\n",
    "\n",
    "        info_list = [li.text.replace(\"\\n\",\"\").strip() for ul in uls for li in ul.find_all(\"li\")]\n",
    "\n",
    "        info_list = lfilter(lambda s: s, info_list)\n",
    "\n",
    "        for info in info_list:\n",
    "            m = re.search(r\"(.*):\\s+(.*)\", info)\n",
    "            if m:\n",
    "                key, val = m.group(1).strip(), m.group(2).strip()\n",
    "                if key == \"Height\":\n",
    "                    val = val.replace(\" \",\"\")\n",
    "                if val == \"--\":\n",
    "                    val = None\n",
    "                fighter_dict[key] = val\n",
    "\n",
    "        return fighter_dict\n",
    "    \n",
    "    def create_fighters_df(self, fighters_html_dir):\n",
    "        fighter_files = os.listdir(fighters_html_dir)\n",
    "        fighter_files = lfilter(lambda s: s.endswith(\".html\"), fighter_files)\n",
    "        fighter_filepaths = lmap(lambda s: os.path.join(fighters_html_dir, s), fighter_files)\n",
    "\n",
    "        with Pool(cpu_count()) as p:\n",
    "            fighters_lod = p.map(self.extract_fighter_data, fighter_filepaths)\n",
    "\n",
    "        return pd.DataFrame(fighters_lod)\n",
    "    \n",
    "    def save_fighters_df(self):\n",
    "        fighters_df = self.create_fighters_df(dir_dict[\"fighters_html\"])\n",
    "        filepath = os.path.join(dir_dict[\"raw_csv\"], \"fighters.csv\")\n",
    "        fighters_df.to_csv(filepath, index=False)\n",
    "        \n",
    "    def start(self):\n",
    "        self.save_fighterlist_pages()\n",
    "        self.get_fighter_urls_list()\n",
    "        self.save_fighter_pages()\n",
    "        self.save_fighters_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bbd0398-e143-46d5-821c-67e7efc81d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect():\n",
    "\n",
    "    events_collector = EventsCollector()\n",
    "\n",
    "    events_collector.start()\n",
    "\n",
    "    print(\"\\nCompleted Events Collector\\n\")\n",
    "    \n",
    "    fights_collector = FightsCollector()\n",
    "\n",
    "    fights_collector.start()\n",
    "    \n",
    "    print(\"\\nCompleted Fights Collector\\n\")\n",
    "\n",
    "    fighters_collector = FightersCollector()\n",
    "\n",
    "    fighters_collector.start()\n",
    "    \n",
    "    print(\"\\nCompleted Fighters Collector\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c5f2b0d-58a8-4ef3-a378-3cf7e29ab0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f364e496-8f51-4599-8489-c2a870a9b4e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 46218.23it/s]\n",
      "24it [00:00, 48164.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event list length: 624\n",
      "Event list length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [00:00<00:00, 99081.80it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 29157.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed Events Collector\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fight urls list length: 6875\n",
      "Fight urls list length: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6875/6875 [00:00<00:00, 118376.64it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 49445.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "upcoming\n",
      "\n",
      "Completed Fights Collector\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 29867.88it/s]\n",
      "\n",
      "7it [00:00, 38530.35it/s]\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 35187.11it/s]\n",
      "\n",
      "10it [00:00, 17817.77it/s]\n",
      "\n",
      "100%|██████████| 9/9 [00:00<00:00, 22712.84it/s]\n",
      "\n",
      "9it [00:00, 14815.05it/s]\n",
      " 12%|█▏        | 3/26 [00:00<00:01, 22.20it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 29026.33it/s]\n",
      "\n",
      "6it [00:00, 20213.51it/s]\n",
      "\n",
      "100%|██████████| 2/2 [00:00<00:00, 12018.06it/s]\n",
      "\n",
      "2it [00:00, 3530.56it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 25692.52it/s]\n",
      "\n",
      "4it [00:00, 6041.49it/s]\n",
      " 23%|██▎       | 6/26 [00:00<00:00, 22.04it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 38887.59it/s]\n",
      "\n",
      "7it [00:00, 10822.02it/s]\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 34622.79it/s]\n",
      "\n",
      "7it [00:00, 7869.24it/s]\n",
      " 35%|███▍      | 9/26 [00:00<00:00, 23.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 18027.09it/s]\n",
      "\n",
      "3it [00:00, 4995.20it/s]\n",
      "\n",
      "100%|██████████| 5/5 [00:00<00:00, 27962.03it/s]\n",
      "\n",
      "5it [00:00, 5962.90it/s]\n",
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 32640.50it/s]\n",
      "\n",
      "6it [00:00, 9807.41it/s]\n",
      " 46%|████▌     | 12/26 [00:00<00:00, 22.60it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 47089.22it/s]\n",
      "\n",
      "14it [00:00, 18117.94it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 15553.66it/s]\n",
      "\n",
      "3it [00:00, 5260.41it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "\n",
      "1it [00:00, 1868.29it/s]\n",
      " 58%|█████▊    | 15/26 [00:00<00:00, 22.83it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 27210.50it/s]\n",
      "\n",
      "7it [00:00, 9258.95it/s]\n",
      "\n",
      "100%|██████████| 7/7 [00:00<00:00, 28954.76it/s]\n",
      "\n",
      "7it [00:00, 9645.25it/s]\n",
      " 69%|██████▉   | 18/26 [00:00<00:00, 24.22it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 51696.43it/s]\n",
      "\n",
      "15it [00:00, 20055.65it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 26588.30it/s]\n",
      "\n",
      "4it [00:00, 19065.02it/s]\n",
      " 81%|████████  | 21/26 [00:00<00:00, 24.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 19463.13it/s]\n",
      "\n",
      "2it [00:00, 1904.77it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 17278.29it/s]\n",
      "\n",
      "4it [00:00, 18517.90it/s]\n",
      "100%|██████████| 26/26 [00:01<00:00, 25.17it/s]\n",
      "100%|██████████| 175/175 [00:04<00:00, 37.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fighter urls list length: 4178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed Fighters Collector\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ufc-fight-predictor]",
   "language": "python",
   "name": "conda-env-ufc-fight-predictor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
